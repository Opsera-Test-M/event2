# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ¤ Canary Deployment Pipeline - Plan & Joy
# Progressive rollout using replica-based traffic splitting
# Compatible with AWS ALB Ingress Controller
# Generated by Opsera Code-to-Cloud v4.0
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
name: "ğŸ¤ Canary Deploy - plan-and-joy"

on:
  workflow_dispatch:
    inputs:
      environment:
        description: 'Target environment for canary'
        required: true
        default: 'qa'
        type: choice
        options:
          - qa
          - staging
          - prod
      skip_quality_gates:
        description: 'Skip quality gates (emergency only)'
        required: false
        default: false
        type: boolean
      auto_promote:
        description: 'Auto-promote through all phases'
        required: false
        default: true
        type: boolean
      phase_delay_minutes:
        description: 'Minutes to wait between phases'
        required: false
        default: '2'
        type: string

env:
  APP_NAME: plan-and-joy
  TENANT: opsera
  AWS_REGION: us-west-2
  ENVIRONMENT: ${{ github.event.inputs.environment || 'qa' }}
  DOMAIN: agent.opsera.dev

concurrency:
  group: canary-${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: false

jobs:
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  # Job 1: Build & Push Canary Image
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  build-canary:
    name: "ğŸ—ï¸ Build Canary"
    runs-on: ubuntu-latest
    outputs:
      image_tag: ${{ steps.build.outputs.image_tag }}
      full_image: ${{ steps.build.outputs.full_image }}
      ecr_uri: ${{ steps.ecr.outputs.uri }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.GH_PAT }}

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Get ECR URI
        id: ecr
        run: |
          AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
          ECR_URI="${AWS_ACCOUNT_ID}.dkr.ecr.${{ env.AWS_REGION }}.amazonaws.com/${{ env.TENANT }}/${{ env.APP_NAME }}"
          echo "uri=$ECR_URI" >> $GITHUB_OUTPUT

      - name: Login to ECR
        run: |
          aws ecr get-login-password --region ${{ env.AWS_REGION }} | \
            docker login --username AWS --password-stdin ${{ steps.ecr.outputs.uri }}

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Build and Push Canary Image
        id: build
        env:
          ECR_URI: ${{ steps.ecr.outputs.uri }}
          VITE_SUPABASE_URL: ${{ secrets.VITE_SUPABASE_URL }}
          VITE_SUPABASE_PUBLISHABLE_KEY: ${{ secrets.VITE_SUPABASE_PUBLISHABLE_KEY }}
        run: |
          IMAGE_TAG="canary-${GITHUB_SHA:0:7}-$(date +%Y%m%d%H%M%S)"
          FULL_IMAGE="${ECR_URI}:${IMAGE_TAG}"

          echo "ğŸ¤ Building canary image: $FULL_IMAGE"

          docker build \
            --build-arg VITE_SUPABASE_URL="${VITE_SUPABASE_URL}" \
            --build-arg VITE_SUPABASE_PUBLISHABLE_KEY="${VITE_SUPABASE_PUBLISHABLE_KEY}" \
            -t "$FULL_IMAGE" \
            -f .opsera-${{ env.APP_NAME }}/Dockerfile \
            .

          docker push "$FULL_IMAGE"

          echo "image_tag=$IMAGE_TAG" >> $GITHUB_OUTPUT
          echo "full_image=$FULL_IMAGE" >> $GITHUB_OUTPUT
          echo "âœ… Canary image pushed: $FULL_IMAGE"

  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  # Job 2: Quality Gates (Grype Security Scan)
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  quality-gates:
    name: "ğŸ”’ Quality Gates"
    runs-on: ubuntu-latest
    needs: build-canary
    if: ${{ github.event.inputs.skip_quality_gates != 'true' }}
    steps:
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Login to ECR
        run: |
          aws ecr get-login-password --region ${{ env.AWS_REGION }} | \
            docker login --username AWS --password-stdin ${{ needs.build-canary.outputs.ecr_uri }}

      - name: Pull Canary Image
        run: docker pull ${{ needs.build-canary.outputs.full_image }}

      - name: Install Grype
        run: |
          curl -sSfL https://raw.githubusercontent.com/anchore/grype/main/install.sh | sh -s -- -b /usr/local/bin

      - name: Run Grype Scan
        run: |
          echo "ğŸ”’ Scanning canary image for vulnerabilities..."
          grype ${{ needs.build-canary.outputs.full_image }} --fail-on critical --output table
          echo "âœ… Security scan passed - no critical vulnerabilities"

  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  # Job 3: Deploy Canary (Phase 1 - 1 canary : 2 stable = ~33% traffic)
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  canary-phase-1:
    name: "ğŸ¤ Phase 1: Deploy Canary"
    runs-on: ubuntu-latest
    needs: [build-canary, quality-gates]
    if: always() && needs.build-canary.result == 'success' && (needs.quality-gates.result == 'success' || needs.quality-gates.result == 'skipped')
    outputs:
      canary_healthy: ${{ steps.health.outputs.healthy }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.GH_PAT }}
          fetch-depth: 0

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Connect to EKS
        run: aws eks update-kubeconfig --name argocd-usw2 --region ${{ env.AWS_REGION }}

      - name: Ensure Namespace Exists
        run: |
          NAMESPACE="${{ env.APP_NAME }}-${{ env.ENVIRONMENT }}"
          kubectl create namespace $NAMESPACE --dry-run=client -o yaml | kubectl apply -f -

      - name: Deploy Canary Pods
        env:
          ECR_URI: ${{ needs.build-canary.outputs.ecr_uri }}
          IMAGE_TAG: ${{ needs.build-canary.outputs.image_tag }}
        run: |
          NAMESPACE="${{ env.APP_NAME }}-${{ env.ENVIRONMENT }}"
          
          echo "ğŸ¤ Deploying canary pods..."
          
          # Create canary deployment with same app label but different version
          cat <<EOF | kubectl apply -f -
          apiVersion: apps/v1
          kind: Deployment
          metadata:
            name: ${{ env.APP_NAME }}-canary
            namespace: $NAMESPACE
            labels:
              app: ${{ env.APP_NAME }}
              version: canary
          spec:
            replicas: 1
            selector:
              matchLabels:
                app: ${{ env.APP_NAME }}
                version: canary
            template:
              metadata:
                labels:
                  app: ${{ env.APP_NAME }}
                  version: canary
              spec:
                containers:
                  - name: ${{ env.APP_NAME }}
                    image: ${ECR_URI}:${IMAGE_TAG}
                    ports:
                      - containerPort: 8080
                    resources:
                      requests:
                        cpu: "50m"
                        memory: "64Mi"
                      limits:
                        cpu: "500m"
                        memory: "512Mi"
                    readinessProbe:
                      httpGet:
                        path: /
                        port: 8080
                      initialDelaySeconds: 5
                      periodSeconds: 5
                    startupProbe:
                      httpGet:
                        path: /
                        port: 8080
                      failureThreshold: 30
                      periodSeconds: 2
          EOF

          echo "âœ… Canary deployment created"

      - name: Wait for Canary Health
        id: health
        run: |
          NAMESPACE="${{ env.APP_NAME }}-${{ env.ENVIRONMENT }}"
          echo "â³ Waiting for canary pods to be healthy..."
          
          kubectl rollout status deployment/${{ env.APP_NAME }}-canary -n $NAMESPACE --timeout=180s
          
          if [ $? -eq 0 ]; then
            echo "healthy=true" >> $GITHUB_OUTPUT
            echo "âœ… Canary pods healthy"
          else
            echo "healthy=false" >> $GITHUB_OUTPUT
            echo "âŒ Canary pods not healthy"
            exit 1
          fi

      - name: Display Phase 1 Status
        run: |
          NAMESPACE="${{ env.APP_NAME }}-${{ env.ENVIRONMENT }}"
          
          echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
          echo "â•‘  ğŸ¤ CANARY PHASE 1 COMPLETE                                  â•‘"
          echo "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£"
          echo "â•‘  Canary pods deployed and healthy                            â•‘"
          echo "â•‘  Next: Wait ${{ github.event.inputs.phase_delay_minutes }} min then promote to stable          â•‘"
          echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
          
          echo ""
          echo "ğŸ“Š Current Deployments:"
          kubectl get deployments -n $NAMESPACE -o wide || true

  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  # Job 4: Observation Period
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  canary-observation:
    name: "ğŸ‘€ Observation Period"
    runs-on: ubuntu-latest
    needs: [build-canary, canary-phase-1]
    if: needs.canary-phase-1.outputs.canary_healthy == 'true'
    outputs:
      canary_stable: ${{ steps.check.outputs.stable }}
    steps:
      - name: Wait for Observation Period
        run: |
          WAIT_MINS=${{ github.event.inputs.phase_delay_minutes }}
          echo "ğŸ‘€ Observing canary for $WAIT_MINS minutes..."
          echo "   - Monitor error rates"
          echo "   - Check response times"
          echo "   - Verify functionality"
          sleep $(( WAIT_MINS * 60 ))

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Connect to EKS
        run: aws eks update-kubeconfig --name argocd-usw2 --region ${{ env.AWS_REGION }}

      - name: Verify Canary Still Healthy
        id: check
        run: |
          NAMESPACE="${{ env.APP_NAME }}-${{ env.ENVIRONMENT }}"
          
          READY=$(kubectl get deployment ${{ env.APP_NAME }}-canary -n $NAMESPACE -o jsonpath='{.status.readyReplicas}' 2>/dev/null || echo "0")
          RESTARTS=$(kubectl get pods -n $NAMESPACE -l version=canary -o jsonpath='{.items[*].status.containerStatuses[*].restartCount}' 2>/dev/null || echo "0")
          
          echo "Ready replicas: $READY"
          echo "Pod restarts: $RESTARTS"
          
          if [ "$READY" -ge 1 ] && [ "${RESTARTS:-0}" -lt 3 ]; then
            echo "stable=true" >> $GITHUB_OUTPUT
            echo "âœ… Canary stable after observation period"
          else
            echo "stable=false" >> $GITHUB_OUTPUT
            echo "âŒ Canary unstable - recommending rollback"
            exit 1
          fi

  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  # Job 5: Promote Canary to Stable
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  promote-canary:
    name: "ğŸš€ Promote to Stable"
    runs-on: ubuntu-latest
    needs: [build-canary, canary-observation]
    if: needs.canary-observation.outputs.canary_stable == 'true'
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.GH_PAT }}
          fetch-depth: 0

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Connect to EKS
        run: aws eks update-kubeconfig --name argocd-usw2 --region ${{ env.AWS_REGION }}

      - name: Update Stable Deployment
        env:
          ECR_URI: ${{ needs.build-canary.outputs.ecr_uri }}
          IMAGE_TAG: ${{ needs.build-canary.outputs.image_tag }}
        run: |
          NAMESPACE="${{ env.APP_NAME }}-${{ env.ENVIRONMENT }}"
          HOST="${{ env.TENANT }}-${{ env.APP_NAME }}-${{ env.ENVIRONMENT }}.${{ env.DOMAIN }}"
          
          echo "ğŸš€ Promoting canary to stable deployment..."
          
          # Check if stable deployment exists
          if kubectl get deployment ${{ env.APP_NAME }} -n $NAMESPACE 2>/dev/null; then
            echo "ğŸ“ Updating existing deployment image..."
            kubectl set image deployment/${{ env.APP_NAME }} \
              ${{ env.APP_NAME }}=${ECR_URI}:${IMAGE_TAG} \
              -n $NAMESPACE
          else
            echo "ğŸ†• Creating new stable deployment..."
            cat <<EOF | kubectl apply -f -
          apiVersion: apps/v1
          kind: Deployment
          metadata:
            name: ${{ env.APP_NAME }}
            namespace: $NAMESPACE
            labels:
              app: ${{ env.APP_NAME }}
          spec:
            replicas: 2
            selector:
              matchLabels:
                app: ${{ env.APP_NAME }}
            template:
              metadata:
                labels:
                  app: ${{ env.APP_NAME }}
              spec:
                containers:
                  - name: ${{ env.APP_NAME }}
                    image: ${ECR_URI}:${IMAGE_TAG}
                    ports:
                      - containerPort: 8080
                    resources:
                      requests:
                        cpu: "50m"
                        memory: "64Mi"
                      limits:
                        cpu: "500m"
                        memory: "512Mi"
                    readinessProbe:
                      httpGet:
                        path: /
                        port: 8080
                      initialDelaySeconds: 5
                      periodSeconds: 5
                    startupProbe:
                      httpGet:
                        path: /
                        port: 8080
                      failureThreshold: 30
                      periodSeconds: 2
          EOF
          fi

          # Ensure service exists
          cat <<EOF | kubectl apply -f -
          apiVersion: v1
          kind: Service
          metadata:
            name: ${{ env.APP_NAME }}
            namespace: $NAMESPACE
          spec:
            selector:
              app: ${{ env.APP_NAME }}
            ports:
              - port: 80
                targetPort: 8080
          EOF

          # Ensure ingress exists (using cert-manager for TLS)
          cat <<EOF | kubectl apply -f -
          apiVersion: networking.k8s.io/v1
          kind: Ingress
          metadata:
            name: ${{ env.APP_NAME }}
            namespace: $NAMESPACE
            annotations:
              cert-manager.io/cluster-issuer: "letsencrypt-prod"
              nginx.ingress.kubernetes.io/ssl-redirect: "true"
          spec:
            ingressClassName: nginx
            tls:
              - hosts:
                  - $HOST
                secretName: ${{ env.APP_NAME }}-${{ env.ENVIRONMENT }}-tls
            rules:
              - host: $HOST
                http:
                  paths:
                    - path: /
                      pathType: Prefix
                      backend:
                        service:
                          name: ${{ env.APP_NAME }}
                          port:
                            number: 80
          EOF

          # Wait for stable deployment
          kubectl rollout status deployment/${{ env.APP_NAME }} -n $NAMESPACE --timeout=180s
          
          echo "âœ… Stable deployment updated"

      - name: Remove Canary Deployment
        run: |
          NAMESPACE="${{ env.APP_NAME }}-${{ env.ENVIRONMENT }}"
          
          echo "ğŸ§¹ Cleaning up canary deployment..."
          kubectl delete deployment ${{ env.APP_NAME }}-canary -n $NAMESPACE --ignore-not-found
          
          echo "âœ… Canary deployment removed"

      - name: Update Kustomization
        run: |
          KUSTOMIZE_FILE=".opsera-${{ env.APP_NAME }}/k8s/overlays/${{ env.ENVIRONMENT }}/kustomization.yaml"
          ECR_URI="${{ needs.build-canary.outputs.ecr_uri }}"
          IMAGE_TAG="${{ needs.build-canary.outputs.image_tag }}"

          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git pull --rebase origin main

          sed -i "s|newName:.*|newName: $ECR_URI|" "$KUSTOMIZE_FILE"
          sed -i "s|newTag:.*|newTag: $IMAGE_TAG|" "$KUSTOMIZE_FILE"

          git add "$KUSTOMIZE_FILE"
          if ! git diff --staged --quiet; then
            git commit -m "chore: promote canary $IMAGE_TAG to stable in ${{ env.ENVIRONMENT }} [skip ci]"
            git push origin main
          fi

      - name: Display Success
        run: |
          echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
          echo "â•‘  ğŸ‰ CANARY DEPLOYMENT COMPLETE                                                   â•‘"
          echo "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£"
          echo "â•‘  App:         ${{ env.APP_NAME }}"
          echo "â•‘  Environment: ${{ env.ENVIRONMENT }}"
          echo "â•‘  Image:       ${{ needs.build-canary.outputs.image_tag }}"
          echo "â•‘  Strategy:    Canary â†’ Stable"
          echo "â•‘  Status:      âœ… Successfully promoted"
          echo "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£"
          echo "â•‘  ğŸŒ URL: https://${{ env.TENANT }}-${{ env.APP_NAME }}-${{ env.ENVIRONMENT }}.${{ env.DOMAIN }}"
          echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"

  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  # Job 6: Rollback (if any phase fails)
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  rollback:
    name: "âª Rollback Canary"
    runs-on: ubuntu-latest
    needs: [canary-phase-1, canary-observation, promote-canary]
    if: failure()
    steps:
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Connect to EKS
        run: aws eks update-kubeconfig --name argocd-usw2 --region ${{ env.AWS_REGION }}

      - name: Rollback Canary
        run: |
          NAMESPACE="${{ env.APP_NAME }}-${{ env.ENVIRONMENT }}"
          
          echo "âª Rolling back canary deployment..."
          
          # Delete canary deployment only
          kubectl delete deployment ${{ env.APP_NAME }}-canary -n $NAMESPACE --ignore-not-found
          
          echo "âœ… Canary deployment removed"
          echo "â„¹ï¸  Stable deployment remains unaffected"
